{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v__2ks6tsXah"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejK2Vg6Qsc2G"
   },
   "source": [
    "**Mounting Drive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "TQ1Kzk3esgAY",
    "outputId": "e68029f4-bec0-4d6e-f11b-41a805b396d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pEL5HFmuwCU"
   },
   "source": [
    "**Loading required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "G7kBXbrwu1zD",
    "outputId": "0d1a37af-dafc-4793-90bd-b5b373d0bfbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZaoQhpmEstH4"
   },
   "source": [
    "**Reading the datafile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "syzgEl2ysq0j"
   },
   "outputs": [],
   "source": [
    "# Open the file as readonly\n",
    "h5f = h5py.File('/content/drive/My Drive/DLCP/SVHN_single_grey1.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2wWw2nnws4uG"
   },
   "source": [
    "## Splitting Dataset into Train & Test\n",
    "\n",
    "#### Also Normalize the data by dividing with 255.0 (Dividing by 255.0 will ensure that the data remains a floating number instead of returning integer values like 1 & 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "HZfGIdfws5j0",
    "outputId": "99fb669c-81d1-44e3-a947-e5cb08535fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 1024) (42000,)\n",
      "Test set (18000, 1024) (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the training, test and validation set\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1024)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1024)\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "l9Pl6BzLs_pX",
    "outputId": "1b5eff40-d556-49d8-e980-9da27fdeb47a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 1024)\n",
      "(42000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wuDEXkGltBzX",
    "outputId": "26b0550f-3b4a-4899-dc77-46fdff73c21f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 1024)\n",
      "(18000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MN6t6TdwtKuD"
   },
   "source": [
    "**Define the Fully Connected Layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nCsqOl-1tVWe"
   },
   "outputs": [],
   "source": [
    "# The fully connected linear layer does matrix-vector multiplication\n",
    "# of inputs and then add the bias\n",
    "\n",
    "class Linear():\n",
    "    \n",
    "    # this is the constructor that keeps some state\n",
    "    # We have parameters Weights, W\n",
    "    # We have biases, B\n",
    "    # We have the input size & output size\n",
    "    # Size of the images is 28 X 28\n",
    "    # So we have 784 as the input size\n",
    "    # We will have some output size. In our case\n",
    "    # we have 10 classes so our output size will be 10\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "\n",
    "        # We create a matrix of size input_size X output_size\n",
    "        # i.e. our matrix is of 783 X 10\n",
    "        # And we initialize the weights with 0.01\n",
    "        self.W = np.random.randn(in_size, out_size) * 0.01\n",
    "        \n",
    "        # We set the bias to Zeroes\n",
    "        # The size of the bias will have the size of the output\n",
    "        # So it will have the size 1 X 10\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        \n",
    "        # Its a good practice to have the list of params\n",
    "        # for easier iterations\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "        # We need 3 more variables which we will use in the backward functions\n",
    "        # These are the gradients w.r.t weights, bias & the inputs\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None        \n",
    "\n",
    "    # Here you have the input X being passed to the forward function\n",
    "    def forward(self, X):\n",
    "        self.X = X\n",
    "\n",
    "        # Vector Matrix multiplication is performed\n",
    "        # Matrix W of weights multipled with the input values\n",
    "        # Bias is then added\n",
    "        # Size of the bias is same as output size\n",
    "        self.output = np.dot(X, self.W) + self.b\n",
    "        return self.output\n",
    "\n",
    "    # When we want to do the backward\n",
    "    # we need the graident coming in from next layer which\n",
    "    # we call here it as nextgrad\n",
    "    def backward(self, nextgrad):\n",
    "        # Gradients for the weights will be gradients coming in \n",
    "        # from the next layer multiplied by the inputs\n",
    "        # This is the Jacobian turns out to be X in case of gradW\n",
    "        # This is the Jacobian turns out to be W in case of gradInput\n",
    "        self.gradW = np.dot(self.X.T, nextgrad)\n",
    "\n",
    "        # We similar compute the gradients for the bias\n",
    "        self.gradB = np.sum(nextgrad, axis=0)\n",
    "\n",
    "        # This are the gradients we will pass to the previous layer\n",
    "        # This is same as multiplication of transpose W with the gradients\n",
    "        # coming in from the next layer        \n",
    "        self.gradInput = np.dot(nextgrad, self.W.T)\n",
    "\n",
    "        # We return the input gradient first and then the\n",
    "        # gradients w.r.t. the parameters W & the bias\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yfmBss1Ztaex"
   },
   "source": [
    "## **Rectified Linear Unit Activation Layer (RELU)**\n",
    "\n",
    "### RELU is an activation function. The ReLU function returns zero when the argument to the function is less than zero and the function is equal to the argument when the argument is above or equal to zero.\n",
    "\n",
    "### It ranges from [0 to Infinity)\n",
    "\n",
    "### The function and its derivative both are monotonic. The issue is that all the negative values become zero immediately which decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into zero immediately. \n",
    "\n",
    "### That can make us use LeakyRELU. However, in this case we will go ahead using RELU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "19ftOU0XtbLK"
   },
   "outputs": [],
   "source": [
    "# Define a class RELU for the Rectified Linear Activation Layer\n",
    "# Note no gradW or gradB parameters because its an activation function\n",
    "# and has no parameters\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        # RELU has no parameters\n",
    "        # However we will initialize to an empty list      \n",
    "        self.params = []\n",
    "\n",
    "        # We will have this gradInput variable to hold the\n",
    "        # state which we will populate in the backward function\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        # When we have an input, we want\n",
    "        # anything less than zero to be zero i.e y becomes zero\n",
    "        # and anything greater than zero then y becomes x      \n",
    "        self.output = np.maximum(X, 0)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, nextgrad):\n",
    "        # Copy the gradients coming in from next layer      \n",
    "        self.gradInput = nextgrad.copy()\n",
    "        \n",
    "        # What ii does is whenever the output was positive, \n",
    "        # it lets the gradient from the next layer pass through\n",
    "        # and whenever it was negative it blocks it and makes it equal to zero        \n",
    "        self.gradInput[self.output <=0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIBCYgM_tgQe"
   },
   "source": [
    "##**Define Softmax Function**\n",
    "\n",
    "### Softmax Function is a generalization of the logistic function. The softmax function is used in various multiclass classification methods, such as multinomial logistic regression, multiclass linear discriminant analysis, naive Bayes classifiers, and artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLjn8DE7tg8P"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    # For each omponent we simply exponentiate it\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    \n",
    "    # Divide by the sum of all exponentials\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D6r_pWPmtznj"
   },
   "source": [
    "## **Define the Cross Entropy Loss**\n",
    "\n",
    "### The cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set.\n",
    "\n",
    "### In other words, Cross Entropy is the number of bits we'll need if we encode symbols from  y  using the wrong tool  ŷ.\n",
    "\n",
    "### Cross entropy is always larger than entropy; encoding symbols according to the wrong distribution  ŷ will always make us use more bits. The only exception is the trivial case where  y  and  ŷ   are equal, and in this case entropy and cross entropy are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKgmlnKQt0Mk"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def forward(self, X, y):\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y])\n",
    "        loss = cross_entropy[0] / self.m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, X, y):\n",
    "        y_idx = y.argmax()\n",
    "        grad = softmax(X)\n",
    "        grad[range(self.m), y] -= 1\n",
    "        grad /= self.m\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BBD9BiuxG8p"
   },
   "source": [
    "### **Visualizing first ten images with their labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "1QIjPdrrxJEj",
    "outputId": "5a920c63-720a-43b0-e7d7-b133e4cec624"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2UAAABdCAYAAAAlt84OAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOTm7+0qEfwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0089a19c18>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the 1st 10 images in our dataset\n",
    "# along with the labels\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15, 1))\n",
    "for i in range(15):\n",
    "    plt.subplot(1, 15, i+1)\n",
    "    plt.imshow(X_train[i].reshape(32,32), cmap=\"gray\")\n",
    "    plt.title(y_train[i], color='r')\n",
    "    plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s6ERj3g9uNbe"
   },
   "source": [
    "### Define the container NN class that enables the forward prop and backward propagation of the entire network. Note, how this class enables us to add layers of different types and also correctly pass gradients using the chain rule. Add L2 Regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXXlHuHvuOm7"
   },
   "outputs": [],
   "source": [
    "# This class enables the forward prop & backward prop to the network\n",
    "class NN():\n",
    "  \n",
    "  # This is the constructor and it takes Cross Entropy as the loss function\n",
    "  # CrossEntropy() is the function that we have defined earlier\n",
    "  def __init__(self, lossfunc=CrossEntropy(), mode='train'):\n",
    "        \n",
    "        # params[] list to have all all the parameters\n",
    "        # If we have 2 layers, then we will have 4 of parameters\n",
    "        # weights & biases of 1st and 2nd layer\n",
    "        self.params = []\n",
    "        \n",
    "        # We assume all our layers are sequential layers\n",
    "        self.layers = []\n",
    "        \n",
    "        # Out loss function is the Cross Entropy Loss Function\n",
    "        self.loss_func = lossfunc\n",
    "        \n",
    "        # This list will contain the gradients w.r.t. the parameters\n",
    "        # So we can use them to update the gradients during training\n",
    "        self.grads = []\n",
    "        \n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "  # add_layer adds layer of different types to the network  \n",
    "  # the layer parameter to the add_layer passes the kind of layer we want\n",
    "  # We have defined types of layers like Linear Layer, RELU Layer\n",
    "  # This function will append the layers to the NN class\n",
    "  # It will also take the parameters of the layers and put it into params[]\n",
    "  def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "  # This function needs an input X and will give an output X\n",
    "  # The individual forwards of each layer is called\n",
    "  # So the layer.forward() takes X as an input, moves forward through\n",
    "  # the whole layer and gives the output X\n",
    "  # In this function all the layers are iterated through\n",
    "  def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "  # \n",
    "  def backward(self, nextgrad):\n",
    "    \n",
    "        # We start with clearing the gradient parameters\n",
    "        # Note: We do not update the weights in the backward()\n",
    "        # This fucntion simply re-initializes the parameters to an \n",
    "        # empty list\n",
    "        self.clear_grad_param()\n",
    "        \n",
    "        # When we want to do backprop, we want to start\n",
    "        # with the last layer. So we use reversed()\n",
    "        for layer in reversed(self.layers):\n",
    "          \n",
    "            # we call the layer.backward() for each layer\n",
    "            # nextgrad are the graidents which we want to pass to the previous \n",
    "            # layer \n",
    "            # grad are the gradients we use to update the params\n",
    "            # so we store grad to self.grads\n",
    "            # the number of elements in self.grads = self.pramas\n",
    "            nextgrad, grad = layer.backward(nextgrad)\n",
    "            \n",
    "            # The layer.backward will call RELU.backward depending on what\n",
    "            # we have passed. Thr RELU.backward will be called with nextgrad\n",
    "            \n",
    "            self.grads.append(grad)\n",
    "            # We keep doing the above for all layers backward\n",
    "        return self.grads\n",
    "\n",
    "  # This is the actual training step\n",
    "  # takes the input X and the target y\n",
    "  # then does the \"forward\"\n",
    "  # calculates the \"loss\"\n",
    "  # then does the \"backward\"\n",
    "  def train_step(self, X, y):\n",
    "    \n",
    "        # calls the forward() to forward the whole thing\n",
    "        # then we have the scores\n",
    "        out = self.forward(X)\n",
    "        \n",
    "        # pass the scores to the cross entropy function\n",
    "        # get the loss from cross entropy function\n",
    "        loss = self.loss_func.forward(out,y)\n",
    "        \n",
    "        # calling the backward\n",
    "        # to get the input gradients that need to be passed\n",
    "        # to the 2nd layer\n",
    "        # size of the next grad will be same as the last later i.e. 10 in our\n",
    "        # case\n",
    "        nextgrad = self.loss_func.backward(out,y)\n",
    "        \n",
    "        # Calls the entire backward of the network\n",
    "        # so that we have the gradients populated\n",
    "        grads = self.backward(nextgrad)\n",
    "        return loss, grads\n",
    "  \n",
    "  # Once we have completely trained the network\n",
    "  # we call the predict function to get the class\n",
    "  # we return the argmax\n",
    "  # Whichever class has max score \n",
    "  def predict(self, X):\n",
    "        \n",
    "        # gets the class from the forward()\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return np.argmax(p, axis=1)\n",
    "  \n",
    "  # Here we do not do the argmax\n",
    "  # Here we get to see the entire scores\n",
    "  def predict_scores(self, X):\n",
    "        X = self.forward(X)\n",
    "        p = softmax(X)\n",
    "        return p\n",
    "  \n",
    "  # This function simply clears the self.grads\n",
    "  def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvT2oxeuuU30"
   },
   "source": [
    "### Define the update function (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "82BcRmaruV-U"
   },
   "outputs": [],
   "source": [
    "def update(velocity, params, grads, learning_rate=0.01, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = (mu * v[i]) - (learning_rate * g[i])\n",
    "            p[i] += v[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjNlNoLruWHq"
   },
   "source": [
    " ### Define a function which gives us the minibatches (both the datapoint and the corresponding label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jxWr6dzruYtn"
   },
   "outputs": [],
   "source": [
    "# get minibatches\n",
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    permutation = np.random.permutation(X.shape[0])\n",
    "    X = X[permutation]\n",
    "    y = y[permutation]\n",
    "    \n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, :]\n",
    "        y_batch = y[i:i + minibatch_size, ]\n",
    "\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPH6TxCpuaX2"
   },
   "source": [
    "### The traning loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3iSYX_ZubV2"
   },
   "outputs": [],
   "source": [
    "def sgd(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9, X_val=None, y_val=None, Lambda=0, verb=True):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "    \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "            \n",
    "        # iterate over mini batches\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update(velocity, net.params, grads, learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "        \n",
    "        # accuracy of model at end of epoch after all mini batch updates\n",
    "        m_train = X_train.shape[0]\n",
    "        m_val = X_val.shape[0]\n",
    "        y_train_pred = []\n",
    "        y_val_pred = []\n",
    "        y_train1 = []\n",
    "        y_vall = []\n",
    "        for ii in range(0, m_train, minibatch_size):\n",
    "            X_tr = X_train[ii:ii + minibatch_size, : ]\n",
    "            y_tr = y_train[ii:ii + minibatch_size,]\n",
    "            y_train1 = np.append(y_train1, y_tr)\n",
    "            y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "        for ii in range(0, m_val, minibatch_size):\n",
    "            X_va = X_val[ii:ii + minibatch_size, : ]\n",
    "            y_va = y_val[ii:ii + minibatch_size,]\n",
    "            y_vall = np.append(y_vall, y_va)\n",
    "            y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "            \n",
    "        train_acc = check_accuracy(y_train1, y_train_pred)\n",
    "        val_acc = check_accuracy(y_vall, y_val_pred)\n",
    "        \n",
    "        ## weights\n",
    "        w = np.array(net.params[0][0])\n",
    "        \n",
    "        ## adding regularization to cost\n",
    "        mean_train_loss = (sum(loss_batch) / float(len(loss_batch)))\n",
    "        mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "        \n",
    "        val_loss_epoch.append(mean_val_loss)\n",
    "        if verb:\n",
    "            if i%50==0:\n",
    "                print(\"Epoch {3}/{4}: Loss = {0} | Training Accuracy = {1}\".format(mean_train_loss, train_acc, val_acc, i, epoch))\n",
    "    return net, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xF10qepsudJo"
   },
   "source": [
    "### Checking the accuracy of the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q5qyyNqqud-u"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(y_true, y_pred):\n",
    "    count = 0\n",
    "    for i,j in zip(y_true, y_pred):\n",
    "        if int(i)==j:\n",
    "            count +=1\n",
    "    return float(count)/float(len(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "59wCRZJEGndM",
    "outputId": "5b36953a-c617-41ef-8764-340fd22c2663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 1024)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LVQcksSFufvh"
   },
   "source": [
    "### Invoking all that we have created until now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EcMZswAuugnY"
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "\n",
    "## input size\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "def train_and_test_loop(iterations, lr, Lambda, verb=True):\n",
    "    ## hyperparameters\n",
    "    iterations = iterations\n",
    "    learning_rate = lr\n",
    "    hidden_nodes = 10\n",
    "    output_nodes = 10\n",
    "\n",
    "    ## define neural net\n",
    "    nn = NN()\n",
    "    nn.add_layer(Linear(input_dim, hidden_nodes))\n",
    "\n",
    "    nn, val_acc = sgd(nn, X_train , y_train, minibatch_size=1000, epoch=iterations, learning_rate=learning_rate,\\\n",
    "                      X_val=X_test, y_val=y_test, Lambda=Lambda, verb=verb)\n",
    "    return val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pTF02rdDuiRr"
   },
   "source": [
    "### Double Check that the loss is reasonable : Disable the regularization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "d0RaMFDmujA3",
    "outputId": "d9d2bf4e-bb1b-43ef-cb67-5d33a31f174d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 0.0022953103716985518 | Training Accuracy = 0.10354761904761904\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10516666666666667"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 0\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SA4wYvig8i-y"
   },
   "source": [
    "### Now, lets crank up the Lambda(Regularization)and check what it does to our loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Fl2W1FVg8mua",
    "outputId": "305bfe9f-2310-42da-b614-0b0c78bed290"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1: Loss = 0.002290574556999201 | Training Accuracy = 0.10066666666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10283333333333333"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "Lambda = 1e3\n",
    "train_and_test_loop(1, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzpzJyLh8oUB"
   },
   "source": [
    "### Now, lets overfit to a small subset of our dataset, in this case 20 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "MTyTBm8Y8qeQ",
    "outputId": "eb2de761-31d3-4c98-e336-28ea982a053f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1024)\n",
      "(20,)\n"
     ]
    }
   ],
   "source": [
    "X_train_subset = X_train[0:20]\n",
    "y_train_subset = y_train[0:20]\n",
    "X_train = X_train_subset\n",
    "y_train = y_train_subset\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dixkUn4U8yiw"
   },
   "source": [
    "### Tip: Make sure that you can overfit very small portion of the training data\n",
    "So, set a small learning rate and turn regularization off\n",
    "\n",
    "In the code below:\n",
    "- Take the first 20 examples from MNIST\n",
    "- turn off regularization(reg=0.0)\n",
    "- use simple vanilla 'sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3434
    },
    "colab_type": "code",
    "id": "5-f3eui681Qv",
    "outputId": "a4e72f8f-b252-4872-c158-21657e9eace7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10000: Loss = 0.1153187839055569 | Training Accuracy = 0.15\n",
      "Epoch 50/10000: Loss = 0.11029793766366283 | Training Accuracy = 0.2\n",
      "Epoch 100/10000: Loss = 0.1063778994313391 | Training Accuracy = 0.2\n",
      "Epoch 150/10000: Loss = 0.10328278684699332 | Training Accuracy = 0.2\n",
      "Epoch 200/10000: Loss = 0.1007869880883187 | Training Accuracy = 0.2\n",
      "Epoch 250/10000: Loss = 0.09871880450859835 | Training Accuracy = 0.15\n",
      "Epoch 300/10000: Loss = 0.09695482732486 | Training Accuracy = 0.2\n",
      "Epoch 350/10000: Loss = 0.09541031479573828 | Training Accuracy = 0.15\n",
      "Epoch 400/10000: Loss = 0.0940290741148464 | Training Accuracy = 0.25\n",
      "Epoch 450/10000: Loss = 0.09277467459989402 | Training Accuracy = 0.25\n",
      "Epoch 500/10000: Loss = 0.09162363890865667 | Training Accuracy = 0.3\n",
      "Epoch 550/10000: Loss = 0.09056057953577584 | Training Accuracy = 0.25\n",
      "Epoch 600/10000: Loss = 0.08957494330387386 | Training Accuracy = 0.3\n",
      "Epoch 650/10000: Loss = 0.0886589527660627 | Training Accuracy = 0.3\n",
      "Epoch 700/10000: Loss = 0.0878063743903937 | Training Accuracy = 0.3\n",
      "Epoch 750/10000: Loss = 0.08701182461373395 | Training Accuracy = 0.3\n",
      "Epoch 800/10000: Loss = 0.08627040735202048 | Training Accuracy = 0.3\n",
      "Epoch 850/10000: Loss = 0.08557754461059135 | Training Accuracy = 0.3\n",
      "Epoch 900/10000: Loss = 0.08492891217070322 | Training Accuracy = 0.3\n",
      "Epoch 950/10000: Loss = 0.08432042702108498 | Training Accuracy = 0.3\n",
      "Epoch 1000/10000: Loss = 0.08374825590200968 | Training Accuracy = 0.3\n",
      "Epoch 1050/10000: Loss = 0.08320882854219139 | Training Accuracy = 0.3\n",
      "Epoch 1100/10000: Loss = 0.08269884769558866 | Training Accuracy = 0.3\n",
      "Epoch 1150/10000: Loss = 0.08221529295738947 | Training Accuracy = 0.3\n",
      "Epoch 1200/10000: Loss = 0.08175541794584221 | Training Accuracy = 0.35\n",
      "Epoch 1250/10000: Loss = 0.08131674169128121 | Training Accuracy = 0.35\n",
      "Epoch 1300/10000: Loss = 0.08089703555483034 | Training Accuracy = 0.4\n",
      "Epoch 1350/10000: Loss = 0.08049430706366642 | Training Accuracy = 0.4\n",
      "Epoch 1400/10000: Loss = 0.08010678191038317 | Training Accuracy = 0.4\n",
      "Epoch 1450/10000: Loss = 0.07973288514328014 | Training Accuracy = 0.4\n",
      "Epoch 1500/10000: Loss = 0.07937122233898009 | Training Accuracy = 0.4\n",
      "Epoch 1550/10000: Loss = 0.07902056133200525 | Training Accuracy = 0.4\n",
      "Epoch 1600/10000: Loss = 0.07867981489189166 | Training Accuracy = 0.4\n",
      "Epoch 1650/10000: Loss = 0.07834802459053067 | Training Accuracy = 0.4\n",
      "Epoch 1700/10000: Loss = 0.07802434598866528 | Training Accuracy = 0.4\n",
      "Epoch 1750/10000: Loss = 0.07770803518637906 | Training Accuracy = 0.4\n",
      "Epoch 1800/10000: Loss = 0.07739843672277777 | Training Accuracy = 0.45\n",
      "Epoch 1850/10000: Loss = 0.07709497276982029 | Training Accuracy = 0.45\n",
      "Epoch 1900/10000: Loss = 0.07679713353990215 | Training Accuracy = 0.45\n",
      "Epoch 1950/10000: Loss = 0.07650446881254133 | Training Accuracy = 0.45\n",
      "Epoch 2000/10000: Loss = 0.07621658047930382 | Training Accuracy = 0.45\n",
      "Epoch 2050/10000: Loss = 0.07593311600551447 | Training Accuracy = 0.45\n",
      "Epoch 2100/10000: Loss = 0.0756537627104889 | Training Accuracy = 0.45\n",
      "Epoch 2150/10000: Loss = 0.07537824277359942 | Training Accuracy = 0.45\n",
      "Epoch 2200/10000: Loss = 0.07510630888044927 | Training Accuracy = 0.45\n",
      "Epoch 2250/10000: Loss = 0.07483774043103432 | Training Accuracy = 0.45\n",
      "Epoch 2300/10000: Loss = 0.07457234023953588 | Training Accuracy = 0.45\n",
      "Epoch 2350/10000: Loss = 0.07430993166295728 | Training Accuracy = 0.45\n",
      "Epoch 2400/10000: Loss = 0.07405035610299059 | Training Accuracy = 0.45\n",
      "Epoch 2450/10000: Loss = 0.07379347083214813 | Training Accuracy = 0.45\n",
      "Epoch 2500/10000: Loss = 0.07353914710125564 | Training Accuracy = 0.45\n",
      "Epoch 2550/10000: Loss = 0.07328726849086471 | Training Accuracy = 0.45\n",
      "Epoch 2600/10000: Loss = 0.07303772947400951 | Training Accuracy = 0.45\n",
      "Epoch 2650/10000: Loss = 0.07279043416204142 | Training Accuracy = 0.45\n",
      "Epoch 2700/10000: Loss = 0.07254529520905859 | Training Accuracy = 0.45\n",
      "Epoch 2750/10000: Loss = 0.07230223285376032 | Training Accuracy = 0.45\n",
      "Epoch 2800/10000: Loss = 0.07206117408043616 | Training Accuracy = 0.5\n",
      "Epoch 2850/10000: Loss = 0.07182205188330264 | Training Accuracy = 0.5\n",
      "Epoch 2900/10000: Loss = 0.0715848046205648 | Training Accuracy = 0.5\n",
      "Epoch 2950/10000: Loss = 0.07134937544644812 | Training Accuracy = 0.5\n",
      "Epoch 3000/10000: Loss = 0.0711157118110592 | Training Accuracy = 0.5\n",
      "Epoch 3050/10000: Loss = 0.07088376501932041 | Training Accuracy = 0.5\n",
      "Epoch 3100/10000: Loss = 0.07065348984141614 | Training Accuracy = 0.5\n",
      "Epoch 3150/10000: Loss = 0.0704248441682175 | Training Accuracy = 0.5\n",
      "Epoch 3200/10000: Loss = 0.07019778870603052 | Training Accuracy = 0.5\n",
      "Epoch 3250/10000: Loss = 0.0699722867057747 | Training Accuracy = 0.5\n",
      "Epoch 3300/10000: Loss = 0.06974830372234808 | Training Accuracy = 0.5\n",
      "Epoch 3350/10000: Loss = 0.06952580740049649 | Training Accuracy = 0.5\n",
      "Epoch 3400/10000: Loss = 0.06930476728398852 | Training Accuracy = 0.5\n",
      "Epoch 3450/10000: Loss = 0.06908515464531054 | Training Accuracy = 0.5\n",
      "Epoch 3500/10000: Loss = 0.06886694233345747 | Training Accuracy = 0.55\n",
      "Epoch 3550/10000: Loss = 0.0686501046377018 | Training Accuracy = 0.55\n",
      "Epoch 3600/10000: Loss = 0.0684346171654927 | Training Accuracy = 0.55\n",
      "Epoch 3650/10000: Loss = 0.068220456732866 | Training Accuracy = 0.55\n",
      "Epoch 3700/10000: Loss = 0.06800760126594776 | Training Accuracy = 0.55\n",
      "Epoch 3750/10000: Loss = 0.06779602971230703 | Training Accuracy = 0.55\n",
      "Epoch 3800/10000: Loss = 0.0675857219610643 | Training Accuracy = 0.55\n",
      "Epoch 3850/10000: Loss = 0.06737665877079368 | Training Accuracy = 0.55\n",
      "Epoch 3900/10000: Loss = 0.06716882170437008 | Training Accuracy = 0.55\n",
      "Epoch 3950/10000: Loss = 0.0669621930700144 | Training Accuracy = 0.55\n",
      "Epoch 4000/10000: Loss = 0.06675675586787441 | Training Accuracy = 0.55\n",
      "Epoch 4050/10000: Loss = 0.06655249374155739 | Training Accuracy = 0.55\n",
      "Epoch 4100/10000: Loss = 0.06634939093409556 | Training Accuracy = 0.55\n",
      "Epoch 4150/10000: Loss = 0.06614743224788576 | Training Accuracy = 0.55\n",
      "Epoch 4200/10000: Loss = 0.06594660300819445 | Training Accuracy = 0.55\n",
      "Epoch 4250/10000: Loss = 0.06574688902986617 | Training Accuracy = 0.55\n",
      "Epoch 4300/10000: Loss = 0.06554827658691227 | Training Accuracy = 0.55\n",
      "Epoch 4350/10000: Loss = 0.06535075238469214 | Training Accuracy = 0.55\n",
      "Epoch 4400/10000: Loss = 0.06515430353443187 | Training Accuracy = 0.55\n",
      "Epoch 4450/10000: Loss = 0.06495891752984963 | Training Accuracy = 0.55\n",
      "Epoch 4500/10000: Loss = 0.0647645822256843 | Training Accuracy = 0.55\n",
      "Epoch 4550/10000: Loss = 0.06457128581794504 | Training Accuracy = 0.55\n",
      "Epoch 4600/10000: Loss = 0.06437901682571655 | Training Accuracy = 0.55\n",
      "Epoch 4650/10000: Loss = 0.06418776407437464 | Training Accuracy = 0.55\n",
      "Epoch 4700/10000: Loss = 0.06399751668008015 | Training Accuracy = 0.55\n",
      "Epoch 4750/10000: Loss = 0.06380826403543259 | Training Accuracy = 0.55\n",
      "Epoch 4800/10000: Loss = 0.06361999579617876 | Training Accuracy = 0.55\n",
      "Epoch 4850/10000: Loss = 0.06343270186887941 | Training Accuracy = 0.55\n",
      "Epoch 4900/10000: Loss = 0.06324637239944951 | Training Accuracy = 0.55\n",
      "Epoch 4950/10000: Loss = 0.06306099776249384 | Training Accuracy = 0.55\n",
      "Epoch 5000/10000: Loss = 0.06287656855136917 | Training Accuracy = 0.55\n",
      "Epoch 5050/10000: Loss = 0.06269307556890955 | Training Accuracy = 0.55\n",
      "Epoch 5100/10000: Loss = 0.06251050981875832 | Training Accuracy = 0.55\n",
      "Epoch 5150/10000: Loss = 0.06232886249725559 | Training Accuracy = 0.55\n",
      "Epoch 5200/10000: Loss = 0.062148124985834705 | Training Accuracy = 0.55\n",
      "Epoch 5250/10000: Loss = 0.0619682888438864 | Training Accuracy = 0.55\n",
      "Epoch 5300/10000: Loss = 0.061789345802051865 | Training Accuracy = 0.55\n",
      "Epoch 5350/10000: Loss = 0.06161128775591076 | Training Accuracy = 0.55\n",
      "Epoch 5400/10000: Loss = 0.061434106760033344 | Training Accuracy = 0.55\n",
      "Epoch 5450/10000: Loss = 0.061257795022367376 | Training Accuracy = 0.55\n",
      "Epoch 5500/10000: Loss = 0.061082344898935284 | Training Accuracy = 0.55\n",
      "Epoch 5550/10000: Loss = 0.06090774888881739 | Training Accuracy = 0.55\n",
      "Epoch 5600/10000: Loss = 0.06073399962939964 | Training Accuracy = 0.55\n",
      "Epoch 5650/10000: Loss = 0.06056108989186787 | Training Accuracy = 0.55\n",
      "Epoch 5700/10000: Loss = 0.0603890125769288 | Training Accuracy = 0.55\n",
      "Epoch 5750/10000: Loss = 0.06021776071074349 | Training Accuracy = 0.55\n",
      "Epoch 5800/10000: Loss = 0.060047327441057397 | Training Accuracy = 0.55\n",
      "Epoch 5850/10000: Loss = 0.05987770603351379 | Training Accuracy = 0.6\n",
      "Epoch 5900/10000: Loss = 0.059708889868138534 | Training Accuracy = 0.6\n",
      "Epoch 5950/10000: Loss = 0.05954087243598406 | Training Accuracy = 0.6\n",
      "Epoch 6000/10000: Loss = 0.059373647335923485 | Training Accuracy = 0.6\n",
      "Epoch 6050/10000: Loss = 0.05920720827158353 | Training Accuracy = 0.6\n",
      "Epoch 6100/10000: Loss = 0.05904154904840937 | Training Accuracy = 0.6\n",
      "Epoch 6150/10000: Loss = 0.05887666357085197 | Training Accuracy = 0.6\n",
      "Epoch 6200/10000: Loss = 0.058712545839671046 | Training Accuracy = 0.6\n",
      "Epoch 6250/10000: Loss = 0.058549189949346804 | Training Accuracy = 0.6\n",
      "Epoch 6300/10000: Loss = 0.058386590085593916 | Training Accuracy = 0.6\n",
      "Epoch 6350/10000: Loss = 0.058224740522972364 | Training Accuracy = 0.6\n",
      "Epoch 6400/10000: Loss = 0.05806363562258867 | Training Accuracy = 0.6\n",
      "Epoch 6450/10000: Loss = 0.057903269829884176 | Training Accuracy = 0.65\n",
      "Epoch 6500/10000: Loss = 0.057743637672503886 | Training Accuracy = 0.65\n",
      "Epoch 6550/10000: Loss = 0.05758473375824337 | Training Accuracy = 0.65\n",
      "Epoch 6600/10000: Loss = 0.057426552773068004 | Training Accuracy = 0.65\n",
      "Epoch 6650/10000: Loss = 0.05726908947920203 | Training Accuracy = 0.65\n",
      "Epoch 6700/10000: Loss = 0.05711233871328388 | Training Accuracy = 0.65\n",
      "Epoch 6750/10000: Loss = 0.056956295384583654 | Training Accuracy = 0.65\n",
      "Epoch 6800/10000: Loss = 0.056800954473280395 | Training Accuracy = 0.65\n",
      "Epoch 6850/10000: Loss = 0.0566463110287969 | Training Accuracy = 0.65\n",
      "Epoch 6900/10000: Loss = 0.056492360168188 | Training Accuracy = 0.65\n",
      "Epoch 6950/10000: Loss = 0.056339097074581124 | Training Accuracy = 0.65\n",
      "Epoch 7000/10000: Loss = 0.056186516995666035 | Training Accuracy = 0.65\n",
      "Epoch 7050/10000: Loss = 0.05603461524223206 | Training Accuracy = 0.65\n",
      "Epoch 7100/10000: Loss = 0.055883387186750566 | Training Accuracy = 0.65\n",
      "Epoch 7150/10000: Loss = 0.05573282826200073 | Training Accuracy = 0.65\n",
      "Epoch 7200/10000: Loss = 0.055582933959736604 | Training Accuracy = 0.65\n",
      "Epoch 7250/10000: Loss = 0.05543369982939465 | Training Accuracy = 0.65\n",
      "Epoch 7300/10000: Loss = 0.05528512147683874 | Training Accuracy = 0.65\n",
      "Epoch 7350/10000: Loss = 0.055137194563142276 | Training Accuracy = 0.65\n",
      "Epoch 7400/10000: Loss = 0.05498991480340546 | Training Accuracy = 0.65\n",
      "Epoch 7450/10000: Loss = 0.054843277965606584 | Training Accuracy = 0.65\n",
      "Epoch 7500/10000: Loss = 0.054697279869485894 | Training Accuracy = 0.65\n",
      "Epoch 7550/10000: Loss = 0.054551916385460676 | Training Accuracy = 0.65\n",
      "Epoch 7600/10000: Loss = 0.054407183433570705 | Training Accuracy = 0.65\n",
      "Epoch 7650/10000: Loss = 0.054263076982453175 | Training Accuracy = 0.65\n",
      "Epoch 7700/10000: Loss = 0.054119593048345016 | Training Accuracy = 0.65\n",
      "Epoch 7750/10000: Loss = 0.053976727694113116 | Training Accuracy = 0.65\n",
      "Epoch 7800/10000: Loss = 0.053834477028310124 | Training Accuracy = 0.65\n",
      "Epoch 7850/10000: Loss = 0.053692837204255696 | Training Accuracy = 0.65\n",
      "Epoch 7900/10000: Loss = 0.0535518044191425 | Training Accuracy = 0.65\n",
      "Epoch 7950/10000: Loss = 0.05341137491316489 | Training Accuracy = 0.65\n",
      "Epoch 8000/10000: Loss = 0.05327154496867179 | Training Accuracy = 0.65\n",
      "Epoch 8050/10000: Loss = 0.05313231090934025 | Training Accuracy = 0.65\n",
      "Epoch 8100/10000: Loss = 0.052993669099371324 | Training Accuracy = 0.65\n",
      "Epoch 8150/10000: Loss = 0.05285561594270575 | Training Accuracy = 0.65\n",
      "Epoch 8200/10000: Loss = 0.05271814788226078 | Training Accuracy = 0.65\n",
      "Epoch 8250/10000: Loss = 0.052581261399185286 | Training Accuracy = 0.65\n",
      "Epoch 8300/10000: Loss = 0.05244495301213421 | Training Accuracy = 0.65\n",
      "Epoch 8350/10000: Loss = 0.05230921927656111 | Training Accuracy = 0.65\n",
      "Epoch 8400/10000: Loss = 0.052174056784028366 | Training Accuracy = 0.65\n",
      "Epoch 8450/10000: Loss = 0.05203946216153458 | Training Accuracy = 0.65\n",
      "Epoch 8500/10000: Loss = 0.051905432070858216 | Training Accuracy = 0.65\n",
      "Epoch 8550/10000: Loss = 0.05177196320791799 | Training Accuracy = 0.65\n",
      "Epoch 8600/10000: Loss = 0.051639052302148214 | Training Accuracy = 0.65\n",
      "Epoch 8650/10000: Loss = 0.051506696115889816 | Training Accuracy = 0.65\n",
      "Epoch 8700/10000: Loss = 0.05137489144379591 | Training Accuracy = 0.65\n",
      "Epoch 8750/10000: Loss = 0.05124363511225157 | Training Accuracy = 0.65\n",
      "Epoch 8800/10000: Loss = 0.05111292397880809 | Training Accuracy = 0.65\n",
      "Epoch 8850/10000: Loss = 0.05098275493162978 | Training Accuracy = 0.65\n",
      "Epoch 8900/10000: Loss = 0.05085312488895477 | Training Accuracy = 0.65\n",
      "Epoch 8950/10000: Loss = 0.05072403079856826 | Training Accuracy = 0.65\n",
      "Epoch 9000/10000: Loss = 0.05059546963728832 | Training Accuracy = 0.65\n",
      "Epoch 9050/10000: Loss = 0.050467438410463514 | Training Accuracy = 0.65\n",
      "Epoch 9100/10000: Loss = 0.050339934151482434 | Training Accuracy = 0.65\n",
      "Epoch 9150/10000: Loss = 0.05021295392129478 | Training Accuracy = 0.65\n",
      "Epoch 9200/10000: Loss = 0.050086494807943305 | Training Accuracy = 0.65\n",
      "Epoch 9250/10000: Loss = 0.04996055392610722 | Training Accuracy = 0.65\n",
      "Epoch 9300/10000: Loss = 0.04983512841665491 | Training Accuracy = 0.65\n",
      "Epoch 9350/10000: Loss = 0.04971021544620823 | Training Accuracy = 0.65\n",
      "Epoch 9400/10000: Loss = 0.0495858122067162 | Training Accuracy = 0.65\n",
      "Epoch 9450/10000: Loss = 0.049461915915037916 | Training Accuracy = 0.65\n",
      "Epoch 9500/10000: Loss = 0.049338523812536256 | Training Accuracy = 0.65\n",
      "Epoch 9550/10000: Loss = 0.049215633164679404 | Training Accuracy = 0.65\n",
      "Epoch 9600/10000: Loss = 0.04909324126065237 | Training Accuracy = 0.65\n",
      "Epoch 9650/10000: Loss = 0.04897134541297659 | Training Accuracy = 0.65\n",
      "Epoch 9700/10000: Loss = 0.0488499429571385 | Training Accuracy = 0.65\n",
      "Epoch 9750/10000: Loss = 0.048729031251226365 | Training Accuracy = 0.65\n",
      "Epoch 9800/10000: Loss = 0.048608607675575 | Training Accuracy = 0.65\n",
      "Epoch 9850/10000: Loss = 0.048488669632418296 | Training Accuracy = 0.65\n",
      "Epoch 9900/10000: Loss = 0.04836921454555021 | Training Accuracy = 0.65\n",
      "Epoch 9950/10000: Loss = 0.048250239859992175 | Training Accuracy = 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1126111111111111"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 0.0001\n",
    "Lambda = 0\n",
    "train_and_test_loop(10000, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z8dp-HzF848y"
   },
   "source": [
    "### Loading the original dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "uEDCPfxA85kj",
    "outputId": "65179f18-307d-4650-eaee-40e5e39f7a67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (42000, 1024) (42000,)\n",
      "Test set (18000, 1024) (18000,)\n"
     ]
    }
   ],
   "source": [
    "# Open the file as readonly\n",
    "h5f = h5py.File('/content/drive/My Drive/DLCP/SVHN_single_grey1.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 1024)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1024)\n",
    "\n",
    "# # normalize inputs from 0-255 to 0-1\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)\n",
    "\n",
    "X_val = X_test\n",
    "y_val = y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JU3kpjenLgCO"
   },
   "source": [
    "### Start with small regularization and find learning rate that makes the loss go down.\n",
    "\n",
    "- we start with Lambda(small regularization) = 1e-7\n",
    "- we start with a small learning rate = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "wksmE-jZLgkA",
    "outputId": "8e643013-14c0-4946-e43c-b3883233563e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = 0.0023200377189507575 | Training Accuracy = 0.1040952380952381\n",
      "Epoch 50/500: Loss = 0.0023191087808429075 | Training Accuracy = 0.1040952380952381\n",
      "Epoch 100/500: Loss = 0.0023182381384005005 | Training Accuracy = 0.10423809523809524\n",
      "Epoch 150/500: Loss = 0.0023174216376722215 | Training Accuracy = 0.10423809523809524\n",
      "Epoch 200/500: Loss = 0.002316655457669191 | Training Accuracy = 0.10426190476190476\n",
      "Epoch 250/500: Loss = 0.002315936080228546 | Training Accuracy = 0.10435714285714286\n",
      "Epoch 300/500: Loss = 0.002315260262976659 | Training Accuracy = 0.10442857142857143\n",
      "Epoch 350/500: Loss = 0.0023146250150301384 | Training Accuracy = 0.10435714285714286\n",
      "Epoch 400/500: Loss = 0.0023140275751201045 | Training Accuracy = 0.1045\n",
      "Epoch 450/500: Loss = 0.0023134653918658508 | Training Accuracy = 0.1045\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09977777777777778"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e-7\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CB16VRAdLiCd"
   },
   "source": [
    "### Okay now lets try a (larger) learning rate 1e6. What could possibly go wrong?\n",
    "\n",
    "- Learning rate lr = 1e6\n",
    "- Regularization lambda = 1e-7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "XbEXx92uLkOl",
    "outputId": "86dd9ad6-7159-4f9c-d69f-c2015a5f4a4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = inf | Training Accuracy = 0.09971428571428571\n",
      "Epoch 50/500: Loss = inf | Training Accuracy = 0.116\n",
      "Epoch 100/500: Loss = inf | Training Accuracy = 0.11333333333333333\n",
      "Epoch 150/500: Loss = inf | Training Accuracy = 0.11335714285714285\n",
      "Epoch 200/500: Loss = inf | Training Accuracy = 0.11442857142857144\n",
      "Epoch 250/500: Loss = inf | Training Accuracy = 0.12202380952380952\n",
      "Epoch 300/500: Loss = inf | Training Accuracy = 0.10323809523809524\n",
      "Epoch 350/500: Loss = inf | Training Accuracy = 0.12333333333333334\n",
      "Epoch 400/500: Loss = inf | Training Accuracy = 0.12714285714285714\n",
      "Epoch 450/500: Loss = inf | Training Accuracy = 0.11719047619047619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.10566666666666667"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = 1e6\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ec6MDOXoLlaD"
   },
   "source": [
    "### Lets try to train now with a value of learning rate between 1e-7 and 1e6\n",
    "\n",
    "- learning rate = 1e4\n",
    "- regularization remains the small, lambda = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tYpe7gwBLnPu",
    "outputId": "b9b43053-d1ab-4e32-d749-c0703d5db2c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/500: Loss = inf | Training Accuracy = 0.09971428571428571\n",
      "Epoch 50/500: Loss = inf | Training Accuracy = 0.11171428571428571\n",
      "Epoch 100/500: Loss = inf | Training Accuracy = 0.09976190476190476\n"
     ]
    }
   ],
   "source": [
    "lr = 1e4\n",
    "Lambda = 1e-7\n",
    "train_and_test_loop(500, lr, Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lnvRVgBLqjS"
   },
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "### Cross validation Strategy\n",
    "\n",
    "\n",
    "- Do coarse -> fine cross-validation in stages\n",
    "\n",
    "- First stage: only a few epochs to get rough idea of what params work\n",
    "- Second stage: longer running time, finer search\n",
    "- … (repeat as necessary)\n",
    "\n",
    "### Tip for detecting explosions in the solver: \n",
    "- If the cost is ever > 3 * original cost, break out early\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9eGp4fxWLtyX"
   },
   "source": [
    "### For example: Run coarse search for 100 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1921
    },
    "colab_type": "code",
    "id": "uWTz0DU_LrAR",
    "outputId": "1453a03c-0096-4832-e04f-e8a62ebfc64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/100: Best_val_acc: 0.09972222222222223, lr: 1.3489071745332914e-07, Lambda: 1.4217723755797894e-05\n",
      "\n",
      "Try 2/100: Best_val_acc: 0.11444444444444445, lr: 1206.9438709660424, Lambda: 0.2051166201934944\n",
      "\n",
      "Try 3/100: Best_val_acc: 0.19833333333333333, lr: 0.002535181651963326, Lambda: 0.00010153483596451206\n",
      "\n",
      "Try 4/100: Best_val_acc: 0.10727777777777778, lr: 1.5422820030954793e-07, Lambda: 28.070398287770843\n",
      "\n",
      "Try 5/100: Best_val_acc: 0.09777777777777778, lr: 1.5007162427104922e-06, Lambda: 0.002883823980340039\n",
      "\n",
      "Try 6/100: Best_val_acc: 0.10511111111111111, lr: 13.746525999681806, Lambda: 5.840504648797076\n",
      "\n",
      "Try 7/100: Best_val_acc: 0.10044444444444445, lr: 3.5921908813020663e-07, Lambda: 0.6192088210981265\n",
      "\n",
      "Try 8/100: Best_val_acc: 0.09533333333333334, lr: 589.4603605678844, Lambda: 0.007392762149485296\n",
      "\n",
      "Try 9/100: Best_val_acc: 0.1822777777777778, lr: 0.0015880142105923838, Lambda: 42.242280654106715\n",
      "\n",
      "Try 10/100: Best_val_acc: 0.09861111111111111, lr: 3.806412083767399e-07, Lambda: 0.004697426779181731\n",
      "\n",
      "Try 11/100: Best_val_acc: 0.09861111111111111, lr: 1.1877863927223149e-07, Lambda: 0.005585800079279331\n",
      "\n",
      "Try 12/100: Best_val_acc: 0.10177777777777777, lr: 1.3280205644939187, Lambda: 0.03947706776707995\n",
      "\n",
      "Try 13/100: Best_val_acc: 0.12727777777777777, lr: 605.8130617972048, Lambda: 0.006821389921506487\n",
      "\n",
      "Try 14/100: Best_val_acc: 0.1975, lr: 0.011559272082045212, Lambda: 1.6441203794307924\n",
      "\n",
      "Try 15/100: Best_val_acc: 0.11555555555555555, lr: 71.16538831692735, Lambda: 889.2193452425387\n",
      "\n",
      "Try 16/100: Best_val_acc: 0.092, lr: 7.083765642324073e-07, Lambda: 19.080130715583792\n",
      "\n",
      "Try 17/100: Best_val_acc: 0.10155555555555555, lr: 1.4860998261099477e-07, Lambda: 2.9886978713709155e-05\n",
      "\n",
      "Try 18/100: Best_val_acc: 0.10077777777777777, lr: 22.391151938577476, Lambda: 0.00016886525572273208\n",
      "\n",
      "Try 19/100: Best_val_acc: 0.10361111111111111, lr: 1.317423278328267e-07, Lambda: 47942.023963699234\n",
      "\n",
      "Try 20/100: Best_val_acc: 0.09716666666666667, lr: 0.4024196540075086, Lambda: 22.738912501928855\n",
      "\n",
      "Try 21/100: Best_val_acc: 0.10527777777777778, lr: 1.7923918706594538e-05, Lambda: 0.021810197369721073\n",
      "\n",
      "Try 22/100: Best_val_acc: 0.11844444444444445, lr: 133.06291777196336, Lambda: 16425.8712378732\n",
      "\n",
      "Try 23/100: Best_val_acc: 0.10561111111111111, lr: 2.2197836297479476, Lambda: 0.029587746149811488\n",
      "\n",
      "Try 24/100: Best_val_acc: 0.18794444444444444, lr: 0.025471775608721464, Lambda: 3.8446172224793433\n",
      "\n",
      "Try 25/100: Best_val_acc: 0.19633333333333333, lr: 0.0026037562139819494, Lambda: 0.000582434837879102\n",
      "\n",
      "Try 26/100: Best_val_acc: 0.11522222222222223, lr: 0.318035047275871, Lambda: 21.86229140096069\n",
      "\n",
      "Try 27/100: Best_val_acc: 0.19055555555555556, lr: 0.002299356087287492, Lambda: 0.021158047898812756\n",
      "\n",
      "Try 28/100: Best_val_acc: 0.12422222222222222, lr: 5.118728909985562, Lambda: 3.374373938708428e-05\n",
      "\n",
      "Try 29/100: Best_val_acc: 0.15433333333333332, lr: 790.3684620371738, Lambda: 3.261411569103308\n",
      "\n",
      "Try 30/100: Best_val_acc: 0.103, lr: 2.9979682949704837e-06, Lambda: 1799.16937063392\n",
      "\n",
      "Try 31/100: Best_val_acc: 0.11438888888888889, lr: 44.85307735383675, Lambda: 69342.9930647573\n",
      "\n",
      "Try 32/100: Best_val_acc: 0.19088888888888889, lr: 0.028893261763550587, Lambda: 13495.492563934931\n",
      "\n",
      "Try 33/100: Best_val_acc: 0.12227777777777778, lr: 6.787108905615202e-05, Lambda: 0.000669723080747376\n",
      "\n",
      "Try 34/100: Best_val_acc: 0.10305555555555555, lr: 6.514812878178059e-06, Lambda: 22888.834536411374\n",
      "\n",
      "Try 35/100: Best_val_acc: 0.10061111111111111, lr: 254.90846605027735, Lambda: 5360.553408765889\n",
      "\n",
      "Try 36/100: Best_val_acc: 0.10811111111111112, lr: 17.698578924175962, Lambda: 7.861093792625654e-05\n",
      "\n",
      "Try 37/100: Best_val_acc: 0.15588888888888888, lr: 0.00020240535646826393, Lambda: 0.012446298242762091\n",
      "\n",
      "Try 38/100: Best_val_acc: 0.09877777777777778, lr: 2.8925023321378972e-05, Lambda: 2.7761419796703337\n",
      "\n",
      "Try 39/100: Best_val_acc: 0.21255555555555555, lr: 0.012094917286809006, Lambda: 2.7774438844889304\n",
      "\n",
      "Try 40/100: Best_val_acc: 0.08261111111111111, lr: 2.942468213063271e-06, Lambda: 0.004682716473884088\n",
      "\n",
      "Try 41/100: Best_val_acc: 0.10177777777777777, lr: 4.1073448840626734e-07, Lambda: 0.021898696036617996\n",
      "\n",
      "Try 42/100: Best_val_acc: 0.10172222222222223, lr: 850.9434193841561, Lambda: 0.004424818767731065\n",
      "\n",
      "Try 43/100: Best_val_acc: 0.09277777777777778, lr: 1.1200386624779757e-06, Lambda: 89.72573901191566\n",
      "\n",
      "Try 44/100: Best_val_acc: 0.18838888888888888, lr: 0.0017997418150506837, Lambda: 98.62847948373336\n",
      "\n",
      "Try 45/100: Best_val_acc: 0.19227777777777777, lr: 0.005597986455460089, Lambda: 2.8776059820216997e-05\n",
      "\n",
      "Try 46/100: Best_val_acc: 0.1751111111111111, lr: 0.0005900321519327623, Lambda: 8.782158355265104e-05\n",
      "\n",
      "Try 47/100: Best_val_acc: 0.10577777777777778, lr: 1.4941576202906166, Lambda: 9.218960517846567e-05\n",
      "\n",
      "Try 48/100: Best_val_acc: 0.12661111111111112, lr: 8.46700375711551, Lambda: 407.25838806556254\n",
      "\n",
      "Try 49/100: Best_val_acc: 0.10955555555555556, lr: 1090.4193838284737, Lambda: 533.8828810193874\n",
      "\n",
      "Try 50/100: Best_val_acc: 0.09994444444444445, lr: 3.4074662033401095e-06, Lambda: 3.997543845340793e-05\n",
      "\n",
      "Try 51/100: Best_val_acc: 0.10055555555555555, lr: 771.9153899893001, Lambda: 475.50349381171924\n",
      "\n",
      "Try 52/100: Best_val_acc: 0.19216666666666668, lr: 0.001962006679328266, Lambda: 3.575999867004046\n",
      "\n",
      "Try 53/100: Best_val_acc: 0.11683333333333333, lr: 368.42753133913044, Lambda: 729.943420651967\n",
      "\n",
      "Try 54/100: Best_val_acc: 0.1391111111111111, lr: 0.4317304939998958, Lambda: 0.06636819212790032\n",
      "\n",
      "Try 55/100: Best_val_acc: 0.17872222222222223, lr: 0.0004109982927380435, Lambda: 0.18453947014390593\n",
      "\n",
      "Try 56/100: Best_val_acc: 0.09383333333333334, lr: 1.9323785301165717e-05, Lambda: 55.49778648827816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "for k in range(1,100):\n",
    "    lr = math.pow(10, np.random.uniform(-7.0, 4.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,5))\n",
    "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EPnPlsiLxm9"
   },
   "source": [
    "### Now run finer search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "aeRhWdOdLx8N",
    "outputId": "518cd4f5-b18f-43e7-c4f6-151fad4d3d52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try 1/100: Best_val_acc: 0.18183333333333335, lr: 0.0015627841187523487, Lambda: 3.0963828622801124\n",
      "\n",
      "Try 2/100: Best_val_acc: 0.2066111111111111, lr: 0.008938232491123084, Lambda: 0.000702890261927268\n",
      "\n",
      "Try 3/100: Best_val_acc: 0.16827777777777778, lr: 0.0011501119317044288, Lambda: 0.01734494266611332\n",
      "\n",
      "Try 4/100: Best_val_acc: 0.20305555555555554, lr: 0.00819099708307534, Lambda: 0.0021746911073095175\n",
      "\n",
      "Try 5/100: Best_val_acc: 0.19633333333333333, lr: 0.002049761698052168, Lambda: 0.0026322703638284256\n",
      "\n",
      "Try 6/100: Best_val_acc: 0.19177777777777777, lr: 0.0019861553779131347, Lambda: 0.0012140965999602696\n",
      "\n",
      "Try 7/100: Best_val_acc: 0.19833333333333333, lr: 0.008306278867108984, Lambda: 0.0004152450841152552\n",
      "\n",
      "Try 8/100: Best_val_acc: 0.1988888888888889, lr: 0.0065261317442428645, Lambda: 0.0015696653929476913\n",
      "\n",
      "Try 9/100: Best_val_acc: 0.19922222222222222, lr: 0.0031200446051965454, Lambda: 0.005578884076955325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "for k in range(1,10):\n",
    "    lr = math.pow(10, np.random.uniform(-3.0, -2.0))\n",
    "    Lambda = math.pow(10, np.random.uniform(-5,2))\n",
    "    best_acc = train_and_test_loop(100, lr, Lambda, False)\n",
    "    print(\"Try {0}/{1}: Best_val_acc: {2}, lr: {3}, Lambda: {4}\\n\".format(k, 100, best_acc, lr, Lambda))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Babysitting-SVHN Python Neural Network_milestone3_notebook.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
